---
title: "Data Science HW3"
author: "Nathalie Fadel"
date: "10/12/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(p8105.datasets)
library(httr)
library(jsonlite)
```

#Problem 1
```{r}
#Import dataset
brfss_smart2010 <- p8105.datasets::brfss_smart2010

#clean up variables
brfss_smart2010 = 
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  rename(state = locationabbr, county = locationdesc) %>%
  filter(topic == "Overall Health")

brfss_smart2010$response <- as.factor(brfss_smart2010$response) 
brfss_smart2010$response <- fct_relevel(brfss_smart2010$response, 
                                        c("Excellent", "Very good", "Good", "Fair", "Poor"))
#couldn't pipe this, idk why?

locations <- brfss_smart2010 %>%
  filter(year == "2002") %>%
  distinct(state, county) %>%
  count(state) %>%
  filter(n==7)
  View(locations)
#CT, FL, NC each have 7 locations in 2002

  #line plot  
brfss_smart2010 %>%
  group_by(state, year) %>%
  distinct(county) %>%
  count(state) %>%
  ggplot(aes(x = year, y = n)) +
  geom_line(aes(color = state)) +
  labs(y = "number of locations", caption = "number of locations per state from 2002-2010")

brfss_table =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(year %in% c(2002, 2006, 2010), state == "NY", response == "Excellent") %>%
  mutate(p_exc = data_value / 100) %>%
  group_by(year) %>%
  summarize("Avg proportion of excellent response" = mean(p_exc), 
            "Std dev" = sd(p_exc)) 

#it works but oof it is not pretty
brfss_smart2010 %>%
  group_by(year, state, response) %>%
  summarize(avg_response = mean(data_value / 100)) %>%
  ggplot(aes(x = year, y = avg_response)) + 
  geom_line(aes(color = state)) +
  facet_grid(~response)
```

#Problem 2
```{r}
#import and clean dataset
instacart_data <- p8105.datasets::instacart
janitor::clean_names(instacart_data)

nrow(instacart_data) #1384617 obs
ncol(instacart_data) #15 var

instacart_data %>%
  group_by(order_id) %>%
  distinct(order_id) %>%
  nrow() #131,209 distinct orders

instacart_data %>%
  group_by(product_name) %>%
  distinct(product_name) %>%
  nrow() #39,123 distinct products

instacart_data %>%
  group_by(department) %>%
  distinct(department) %>%
  nrow() #21 distinct departments

orders <- instacart_data %>%
  group_by(aisle, department) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) 
#veg is most ordered aisle - 150,609, followed by fruit - 150,473.
#134 aisles total
  
orders %>%
  group_by(aisle) %>%
  ggplot(aes(x = aisle, y = n, color = department)) +
  coord_flip() +
  geom_col()
#fix this fucking bar chart it sucks

instacart_data %>%
  filter(aisle %in% 
           c('baking ingredients', 'dog food care', 'packaged vegetables fruits')) %>%
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  filter(min_rank(desc(n)) < 2) %>%
  rename("# of orders" = n, "product name" = product_name) %>%
  knitr::kable()

days_of_week <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
instacart_data$order_dow <- days_of_week[instacart_data$order_dow] 
instacart_data$order_dow <- fct_relevel(instacart_data$order_dow, 
                                        c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

instacart_data %>%
  filter(product_name %in% c("Coffee Ice Cream", "Pink Lady Apples")) %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(order_dow, product_name) %>%
  summarize(avg_hod = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = avg_hod) %>%
  knitr::kable()
```

#Problem 3
```{r}
ny_noaa_data <- p8105.datasets::ny_noaa %>%
janitor::clean_names() %>%
  separate(date, c("year", "month", "day"), sep = "-") %>%
  mutate(prcp = prcp / 10, 
         tmax = as.numeric(tmax) / 10,
         tmin = as.numeric(tmin) / 10,
         year = as.integer(year), #needed to add this bc it wasn't recognizing year as a numeric variable. Also, need integer years obvi.
         month = month.name[as.integer(month)])

nrow(ny_noaa_data)
ncol(ny_noaa_data)

ny_noaa_data %>%
  filter_all(any_vars(is.na(.))) %>%
  count() / nrow(ny_noaa_data)
#0.5289595 = proportion of missing values - that's a lot

ny_noaa_data %>%
  summarize(mean(snow, na.rm = TRUE))

ny_noaa_data %>%
  summarize(median(snow, na.rm = TRUE)) 

ny_noaa_data %>%
  summarize(max(snow, na.rm = TRUE))

x <- ny_noaa_data$snow 
y <- table(x)
names(y)[which(y==max(y))]
#most frequent is 0mm of snowfall, obviously

ny_noaa_data %>%
  filter(month %in% c("January", "July")) %>%
  group_by(year, month) %>%
  summarize(avg_high = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = avg_high)) +
  geom_line(color = "red") +
  facet_grid(~month)
#line plot by month   

ny_noaa_data %>%
  filter(month =="January") %>%
  group_by(year) %>%
  summarize(jan_std = sd(tmax, na.rm = TRUE)) %>%
  summarize(max(jan_std))
#8.27 degree std dev

ny_noaa_data %>%
  filter(month =="July") %>%
  group_by(year) %>%
  summarize(july_std = sd(tmax, na.rm = TRUE)) %>%
  summarize(max(july_std))
#4.74 degree std dev. January has higher std dev in temp than July - almost twice as much. High temperature fluctuations in the winter seem to be more drastic from year to year, judging by the graph.







```




---
title: "Data Science HW3"
author: "Nathalie Fadel"
date: "10/12/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(p8105.datasets)
library(httr)
library(jsonlite)
```

#Problem 1

##Data import and cleaning
```{r}
brfss_smart2010 <- p8105.datasets::brfss_smart2010

brfss_smart2010 = 
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  rename(state = locationabbr, county = locationdesc) %>%
  filter(topic == "Overall Health")

brfss_smart2010$response <- as.factor(brfss_smart2010$response) 
brfss_smart2010$response <- 
  fct_relevel(brfss_smart2010$response, 
    c("Excellent", "Very good", "Good", "Fair", "Poor"))
```

##Tables and plots
```{r}
locations <- brfss_smart2010 %>%
  filter(year == "2002") %>%
  distinct(state, county) %>%
  count(state) %>%
  filter(n==7)
  View(locations)
```
From the table, we can see that Connecticut, Florida, and North Carolina all had 7 distinct locations in 2002.

```{r}
brfss_smart2010 %>%
  group_by(state, year) %>%
  distinct(county) %>%
  count(state) %>%
  ggplot(aes(x = year, y = n)) +
  geom_line(aes(color = state)) +
  labs(y = "number of locations", caption = "number of locations per state from 2002-2010")

brfss_table =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(year %in% c(2002, 2006, 2010), state == "NY", response == "Excellent") %>%
  mutate(p_exc = data_value / 100) %>%
  group_by(year) %>%
  summarize("Avg proportion of excellent response" = mean(p_exc), 
            "Std dev" = sd(p_exc)) %>%
  knitr::kable()

brfss_smart2010 %>%
  group_by(year, state, response) %>%
  summarize(avg_response = mean(data_value / 100)) %>%
  ggplot(aes(x = year, y = avg_response)) + 
  geom_line(aes(color = state)) +
  facet_grid(~response)
#fix this
```

#Problem 2
```{r}
instacart_data <- p8105.datasets::instacart
janitor::clean_names(instacart_data)
```
This dataset shows the grocery orders made on the website Instacart. It tells us the order IDs, product IDs, user IDs, items chosen, department, aisle, which day of the week and the hour of day that each order was placed. It also shows how long each user went between orders. There are `r nrow(instacart_data)` observations and `r ncol(instacart_data)` variables in the dataset. There are `r instacart_data %>% group_by(order_id) %>% distinct(order_id) %>% nrow()` distinct orders, `r instacart_data %>% group_by(product_name) %>% distinct(product_name) %>% nrow()` distinct products, and `r instacart_data %>% group_by(department) %>% distinct(department) %>% nrow()` distinct departments in this dataset.

```{r}
orders <- instacart_data %>%
  group_by(aisle, department) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) 
#veg is most ordered aisle - 150,609, followed by fruit - 150,473.
#134 aisles total
  
orders %>%
  group_by(aisle) %>%
  ggplot(aes(x = aisle, y = n, color = department)) +
  coord_flip() +
  geom_col()
#fix this fucking bar chart it sucks ass

instacart_data %>%
  filter(aisle %in% 
           c('baking ingredients', 'dog food care', 'packaged vegetables fruits')) %>%
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  filter(min_rank(desc(n)) < 2) %>%
  rename("# of orders" = n, "product name" = product_name) %>%
  knitr::kable()

days_of_week <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
instacart_data$order_dow <- days_of_week[instacart_data$order_dow] 
instacart_data$order_dow <- fct_relevel(instacart_data$order_dow, 
                                        c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

instacart_data %>%
  filter(product_name %in% c("Coffee Ice Cream", "Pink Lady Apples")) %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(order_dow, product_name) %>%
  summarize(avg_hod = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = avg_hod) %>%
  knitr::kable()
```

#Problem 3
```{r}
ny_noaa_data <- p8105.datasets::ny_noaa %>%
janitor::clean_names() %>%
  separate(date, c("year", "month", "day"), sep = "-") %>%
  mutate(prcp = prcp / 10, 
         tmax = as.numeric(tmax) / 10,
         tmin = as.numeric(tmin) / 10,
         year = as.integer(year), #needed to add this bc it wasn't recognizing year as a numeric variable. Also, need integer years obvi.
         month = month.name[as.integer(month)])

nrow(ny_noaa_data)
ncol(ny_noaa_data)

ny_noaa_data %>%
  filter_all(any_vars(is.na(.))) %>%
  count() / nrow(ny_noaa_data)
#0.5289595 = proportion of missing values - that's a lot

ny_noaa_data %>%
  summarize(mean(snow, na.rm = TRUE))

ny_noaa_data %>%
  summarize(median(snow, na.rm = TRUE)) 

ny_noaa_data %>%
  summarize(max(snow, na.rm = TRUE))

x <- ny_noaa_data$snow 
y <- table(x)
names(y)[which(y==max(y))]
#most frequent is 0mm of snowfall, obviously

ny_noaa_data %>%
  filter(month %in% c("January", "July")) %>%
  group_by(year, month) %>%
  summarize(avg_high = mean(tmax, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = avg_high)) +
  geom_line(color = "red") +
  facet_grid(~month)
#line plot by month   

ny_noaa_data %>%
  filter(month =="January") %>%
  group_by(year) %>%
  summarize(jan_std = sd(tmax, na.rm = TRUE)) %>%
  summarize(max(jan_std))
#8.27 degree std dev

ny_noaa_data %>%
  filter(month =="July") %>%
  group_by(year) %>%
  summarize(july_std = sd(tmax, na.rm = TRUE)) %>%
  summarize(max(july_std))
#4.74 degree std dev. January has higher std dev in temp than July - almost twice as much. High temperature fluctuations in the winter seem to be more drastic from year to year, judging by the graph.

temps = ny_noaa_data %>%
  filter(!is.na(tmin), !is.na(tmax)) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_bin2d() +
  viridis::scale_fill_viridis(option = "inferno")

snowfall = ny_noaa_data %>% 
  filter(snow > 0, snow < 100) %>% 
  mutate(year = as.factor(year)) %>% #had to change year back to a factor vector
  ggplot(aes(x = snow, color = year)) + 
  geom_density(alpha = .05) + 
  viridis::scale_color_viridis(name = "year", discrete = TRUE, option = "inferno")

complete = (temps + snowfall) 

```



